{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd25791",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "This notebook contains additional code for filtering soundings around a tower site. This task is not part of the ARSET training, but may be of interest. Performing this search with the default parameters will take about 2 hours, so the output is pre-computed and stored in the file \"us_me2_oco3_dates.json\"\n",
    "\n",
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "import json\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "from pysif import GesDiscDownloader\n",
    "\n",
    "dl = GesDiscDownloader()\n",
    "\n",
    "def get_local_granule(local_dir: str, dataset: str, d: datetime):\n",
    "    \"\"\"\n",
    "    Get the full path to a granule in the local directory for the specified date and\n",
    "    dataset.\n",
    "\n",
    "    Arguments:\n",
    "        local_dir (str): String path of the source granule directory.\n",
    "        dataset (str): The dataset identifier, not the same as the one used in the OpenDAP portal.\n",
    "        d (datetime): The requested date.\n",
    "\n",
    "    Returns:\n",
    "        DatasetType: A netCDF Dataset object.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: No data is available for the requested day in the dataset\n",
    "    \"\"\"\n",
    "    found_files = glob(\n",
    "        os.path.join(local_dir, f\"{dataset}*{d.strftime('%y%m%d')}*.nc*\")\n",
    "    )\n",
    "    if len(found_files) > 0:\n",
    "        return Dataset(found_files[0], \"r\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Unable to find a matching granule for {d.strftime('%y%m%d')}\"\n",
    "        )\n",
    "\n",
    "def compress_indices(indices: list[int]) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Convert a list of indices to a compact representation using ranges.\n",
    "    \n",
    "    Args:\n",
    "        indices: List of integers (should be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of ints: [start, end] for ranges or [index] for singles\n",
    "    \"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "    \n",
    "    indices = sorted(set(indices))  # Remove duplicates and sort\n",
    "    compressed = []\n",
    "    start = indices[0]\n",
    "    end = indices[0]\n",
    "    \n",
    "    for i in indices[1:]:\n",
    "        if i == end + 1:  # Consecutive\n",
    "            end = i\n",
    "        else:  # Gap found\n",
    "            if start == end:\n",
    "                compressed.append([start])  # Single index\n",
    "            else:\n",
    "                compressed.append([start, end])  # Range\n",
    "            start = end = i\n",
    "    \n",
    "    # Don't forget the last range\n",
    "    if start == end:\n",
    "        compressed.append([start])\n",
    "    else:\n",
    "        compressed.append([start, end])\n",
    "    \n",
    "    return compressed\n",
    "\n",
    "def extract_indices(g, v, ndcs, pydap=True):\n",
    "    parts = []\n",
    "    if pydap:\n",
    "        v = v.replace(\"/\", \"_\")\n",
    "        var = g[v].data\n",
    "    else:\n",
    "        if \"/\" in v:\n",
    "            group = v.split(\"/\")[0]\n",
    "            varname = v.split(\"/\")[-1]\n",
    "            var = g[group][varname]\n",
    "        else:\n",
    "            var = g[v]\n",
    "\n",
    "    for item in ndcs:\n",
    "        if len(item) == 1:\n",
    "            parts.append(var[item[0]:item[0]+1])\n",
    "        else:\n",
    "            start, end = item\n",
    "            parts.append(var[start:end+1])\n",
    "    \n",
    "    return np.concatenate(parts) if parts else np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7699c",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"OCO3_L2_Lite_SIF.11r\"\n",
    "\n",
    "# From the start of the OCO-3 mission to the end of the Ameriflux dataset (at least for the US-Me-2 site)\n",
    "start_date = datetime(2019, 8, 6)\n",
    "end_date = datetime(2021, 12, 31)\n",
    "\n",
    "# Lat/Lon coordinate of the tower site\n",
    "#tower_site = (44.4526, -121.5589)\n",
    "#site_name = \"us_me2\"\n",
    "tower_site = (45.5598, -84.7138)\n",
    "site_name = \"us_umb\"\n",
    "# Amount of area around the tower site to allow in spatial averaging of SIF\n",
    "tolerance = 0.25\n",
    "lat_min = tower_site[0] - tolerance\n",
    "lat_max = tower_site[0] + tolerance\n",
    "lon_min = tower_site[1] - tolerance\n",
    "lon_max = tower_site[1] + tolerance\n",
    "\n",
    "# Create list of dates within the time range\n",
    "current_date = start_date\n",
    "dates: list[datetime] = []\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f224bc",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Please be aware that due to the large time range, this process will take about 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx_obj = {\"dates\": []}\n",
    "\n",
    "for dt in tqdm(dates, desc=\"Studying dates\"):\n",
    "    try:\n",
    "        granule = dl.get_granule_by_date(\"OCO3_L2_Lite_SIF.11r\", dt)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    lat = np.array(granule[\"Latitude\"].data[:])\n",
    "    lon = np.array(granule[\"Longitude\"].data[:])\n",
    "    coords = [(x, y) for x, y in zip(lon, lat)]\n",
    "\n",
    "    # It is more efficient to use np.where, but this method is more intuitive and easier to store in JSON\n",
    "    tower_ndx: list[int] = []\n",
    "    for ndx, coord in enumerate(coords):\n",
    "        if coord[0] > lon_min and coord[0] < lon_max and coord[1] > lat_min and coord[1] < lat_max:\n",
    "            tower_ndx.append(ndx)\n",
    "    if tower_ndx != []:\n",
    "        comp_indices = compress_indices(tower_ndx)\n",
    "        ndx_obj[\"dates\"].append({\"date\": dt.strftime(\"%Y-%m-%d\"), \"indices\": comp_indices})\n",
    "\n",
    "with open(f\"{site_name}_oco3_dates.json\", \"w\") as fp:\n",
    "    json.dump(ndx_obj, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0db50",
   "metadata": {},
   "source": [
    "## Download granules (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48573f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{site_name}_oco3_dates.json\") as fp:\n",
    "    obj = json.load(fp)\n",
    "\n",
    "for day in tqdm(obj[\"dates\"], desc=\"Downloading granules\"):\n",
    "    date_list = day[\"date\"].split(\"-\")\n",
    "    date = datetime(int(date_list[0]), int(date_list[1]), int(date_list[2]))\n",
    "    dl.download_timerange(dataset, date, date, f\"data/{dataset}/{site_name}\", yes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b303cb7",
   "metadata": {},
   "source": [
    "## Compute Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_local = True\n",
    "with open(f\"{site_name}_oco3_dates.json\") as fp:\n",
    "    obj = json.load(fp)\n",
    "\n",
    "daily_avg_sif: list[dict[str, float]] = []\n",
    "for day in tqdm(obj[\"dates\"], desc=\"Computing averages\"):\n",
    "    date_list = day[\"date\"].split(\"-\")\n",
    "    date = datetime(int(date_list[0]), int(date_list[1]), int(date_list[2]))\n",
    "    indices = day[\"indices\"]\n",
    "    if use_local:\n",
    "        granule = get_local_granule(f\"data/{dataset}/{site_name}\", \"oco3_LtSIF\", date)\n",
    "    else:\n",
    "        granule = dl.get_granule_by_date(dataset, date)\n",
    "    sif = extract_indices(granule, \"Daily_SIF_757nm\", indices, pydap=(not use_local))\n",
    "    qual_flag = extract_indices(granule, \"Quality_Flag\", indices, pydap=(not use_local))\n",
    "    igbp_type = extract_indices(granule, \"Science/IGBP_index\", indices, pydap=(not use_local))\n",
    "    # Filter to Deciduous Broadleaf and Mixed Forest Biomes only, Quality flag good or best\n",
    "    cond = (qual_flag < 2) # & ((igbp_type == 4) + (igbp_type == 5))\n",
    "    filtsif = np.where(cond, sif, np.nan)\n",
    "    mean = np.nanmean(filtsif)\n",
    "    if np.isnan(mean):\n",
    "        mean = 0.0\n",
    "    else:\n",
    "        mean = float(mean)\n",
    "    daily_avg_sif.append({\"date\": day[\"date\"], \"sif\": mean})\n",
    "    if use_local:\n",
    "        granule.close()\n",
    "\n",
    "with open(f\"{site_name}_oco3_sif.json\", \"w\") as fp:\n",
    "    json.dump({\"dates\": daily_avg_sif}, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cca46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
