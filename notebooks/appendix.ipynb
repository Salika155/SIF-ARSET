{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd25791",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "This notebook contains additional code for filtering soundings around a tower site. This task is not part of the ARSET training, but may be of interest. Performing this search with the default parameters will take about 2 hours, so the output is pre-computed and stored in the file \"us_me2_oco3_dates.json\"\n",
    "\n",
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compress_indices(indices: list[int]) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Convert a list of indices to a compact representation using ranges.\n",
    "    \n",
    "    Args:\n",
    "        indices: List of integers (should be sorted)\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of ints: [start, end] for ranges or [index] for singles\n",
    "    \"\"\"\n",
    "    if not indices:\n",
    "        return []\n",
    "    \n",
    "    indices = sorted(set(indices))  # Remove duplicates and sort\n",
    "    compressed = []\n",
    "    start = indices[0]\n",
    "    end = indices[0]\n",
    "    \n",
    "    for i in indices[1:]:\n",
    "        if i == end + 1:  # Consecutive\n",
    "            end = i\n",
    "        else:  # Gap found\n",
    "            if start == end:\n",
    "                compressed.append([start])  # Single index\n",
    "            else:\n",
    "                compressed.append([start, end])  # Range\n",
    "            start = end = i\n",
    "    \n",
    "    # Don't forget the last range\n",
    "    if start == end:\n",
    "        compressed.append([start])\n",
    "    else:\n",
    "        compressed.append([start, end])\n",
    "    \n",
    "    return compressed\n",
    "\n",
    "def extract_indices(g, v, ndcs):\n",
    "    parts = []\n",
    "\n",
    "    for item in ndcs:\n",
    "        if len(item) == 1:\n",
    "            parts.append(g[v].data[item[0]:item[0]+1])\n",
    "        else:\n",
    "            start, end = item\n",
    "            parts.append(g[v].data[start:end+1])\n",
    "    \n",
    "    return np.concatenate(parts) if parts else np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7699c",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "dataset = \"OCO3_L2_Lite_SIF.11r\"\n",
    "\n",
    "# From the start of the OCO-3 mission to the end of the Ameriflux dataset (at least for the US-Me-2 site)\n",
    "start_date = datetime(2019, 8, 6)\n",
    "end_date = datetime(2022, 12, 31)\n",
    "\n",
    "# Lat/Lon coordinate of the tower site\n",
    "tower_site = (44.4526, -121.5589)\n",
    "# Amount of area around the tower site to allow in spatial averaging of SIF\n",
    "tolerance = 0.25\n",
    "lat_min = tower_site[0] - tolerance\n",
    "lat_max = tower_site[0] + tolerance\n",
    "lon_min = tower_site[1] - tolerance\n",
    "lon_max = tower_site[1] + tolerance\n",
    "\n",
    "# Create list of dates within the time range\n",
    "current_date = start_date\n",
    "dates: list[datetime] = []\n",
    "while current_date <= end_date:\n",
    "    dates.append(current_date)\n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f224bc",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Please be aware that due to the large time range, this process will take about 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "from pysif import GesDiscDownloader\n",
    "\n",
    "dl = GesDiscDownloader()\n",
    "\n",
    "ndx_obj = {\"dates\": []}\n",
    "\n",
    "for dt in tqdm(dates, desc=\"Studying dates\"):\n",
    "    try:\n",
    "        granule = dl.get_granule_by_date(\"OCO3_L2_Lite_SIF.11r\", dt)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    lat = np.array(granule[\"Latitude\"].data[:])\n",
    "    lon = np.array(granule[\"Longitude\"].data[:])\n",
    "    coords = [(x, y) for x, y in zip(lon, lat)]\n",
    "\n",
    "    # It is more efficient to use np.where, but this method is more intuitive and easier to store in JSON\n",
    "    tower_ndx: list[int] = []\n",
    "    for ndx, coord in enumerate(coords):\n",
    "        if coord[0] > lon_min and coord[0] < lon_max and coord[1] > lat_min and coord[1] < lat_max:\n",
    "            tower_ndx.append(ndx)\n",
    "    if tower_ndx != []:\n",
    "        comp_indices = compress_indices(tower_ndx)\n",
    "        ndx_obj[\"dates\"].append({\"date\": dt.strftime(\"%Y-%m-%d\"), \"indices\": comp_indices})\n",
    "\n",
    "with open(\"us_me2_oco3_dates.json\", \"w\") as fp:\n",
    "    json.dump(ndx_obj, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"us_me2_oco3_dates.json\") as fp:\n",
    "    obj = json.load(fp)\n",
    "\n",
    "daily_avg_sif: list[dict[str, float]] = []\n",
    "for day in tqdm(obj[\"dates\"], desc=\"Computing averages\"):\n",
    "    date_list = day[\"date\"].split(\"-\")\n",
    "    date = datetime(int(date_list[0]), int(date_list[1]), int(date_list[2]))\n",
    "    indices = day[\"indices\"]\n",
    "    granule = dl.get_granule_by_date(dataset, date)\n",
    "    sif = extract_indices(granule, \"Daily_SIF_757nm\", indices)\n",
    "    qual_flag = extract_indices(granule, \"Quality_Flag\", indices)\n",
    "    filtsif = np.where(qual_flag < 2, sif, np.nan)\n",
    "    daily_avg_sif.append({\"date\": day[\"date\"], \"sif\": float(np.nanmean(filtsif))})\n",
    "\n",
    "with open(\"us_me2_oco3_sif.json\", \"w\") as fp:\n",
    "    json.dump({\"dates\": daily_avg_sif}, fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
